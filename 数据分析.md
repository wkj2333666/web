## 数据分析

### 1. 重复上传最多的歌曲
重复上传最多的歌曲（前三）为《你》《情人》《刀马旦》，分别为14次、12次、11次  

完整数据见 [cover_most.csv](./analysis/cover_most.csv)
![](./analysis/most_cover.png)

代码如下：
```python
# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import config
import swifter
from matplotlib.font_manager import FontProperties

# 设置全局中文字体
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = [
    'Noto Sans CJK SC',  # Google Noto 字体
    'Source Han Sans SC', # Adobe 思源黑体
    'WenQuanYi Micro Hei', # 文泉驿微米黑
    'DejaVu Sans',
    'Arial Unicode MS'
]
plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题

# %%
src_file = config.WORKSPACE + 'data/song_search.json'
dest_file = config.WORKSPACE + 'analysis/cover_most.csv'
df_song = pd.read_json(src_file, orient='records', lines=True)

# %%
pattern = re.compile(r'(.+)[（\(].*[）\)]')
def remove_name_suffix(row: pd.Series) -> pd.Series:
    match = re.match(pattern, row.song_name)
    if match is not None:
        row.song_name = match.group(1).strip()
    
    return row

# %%
df_song = df_song.swifter.apply(remove_name_suffix, axis=1)

# %%
df_cover = df_song.groupby(by='song_name').size().sort_values(ascending=False)

# %%
df_cover.to_csv(dest_file, header=False, index=True, encoding='utf-8')

# %%
df_cover = df_cover[df_cover > 5]

# %%
fig = plt.figure()
ax = fig.add_axes([0.1, 0.2, 0.8, 0.7])
ax.set_title('上传次数最多的歌')
bar = ax.bar(df_cover.index, df_cover.values, width=0.5)
ax.bar_label(bar)
plt.xticks(rotation=45, ha='right')
plt.show()

```

#### 逻辑：
在音乐网站中，同一首歌往往会被上传好几次，例如多专辑发行、翻唱（cover）、现场（live）。后两种情况中，歌名往往会在末尾加上括号以标注是翻唱/live版，从而造成歌名不同。  

为此，需要先对歌名进行预处理。先用正则表达式提取出不含括号标注的部分，再用strip()方法去除两端空白字符，如此便可得到原歌名。  

之后，依据歌名进行分组，再统计每组的数量，排序，之后便可以绘制柱状图。这里选取重复数大于5的歌曲进行绘制。 

### 2. 评论区词云分析
根据词云分析，最多的评论关键词为喜欢、好听等积极词汇，反映出评论区氛围较为友好。  
其次，可以看出用词较为情绪化，说明网友听歌时情绪更为放松、随意。  
#### 加入表情符号的停用词
![](./analysis/wordcloud-add-stopwords.jpg)
#### 未加入表情符号的停用词
![](./analysis/wordcloud.jpg)
未加入表情符号的停用词，故出现爱心、流泪、亲亲、大哭等词云  
（源自[爱心]类格式的表情符号）  

代码如下：
```python
# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import wordcloud
import config
import jieba
import swifter
from stopwords import stopwords

# %%
src_file = config.WORKSPACE + 'data/song_info.json'
series_song = pd.read_json(src_file, orient='records', lines=True)['comments']
dest_file = config.WORKSPACE + 'analysis/wordcloud-add-stopwords.jpg'

# %%
frequencies = {}
def get_all_comments(comments: list):
    comment_text = ';'.join([
        comment['content'] for comment in comments
    ])
    comment_text = comment_text.replace('\n', ';')
    global frequencies
    for token in jieba.cut(comment_text):
        if token not in frequencies.keys():
            frequencies[token] = 1
        else:
            frequencies[token] += 1
        

# %%
series_song.swifter.apply(get_all_comments)

# %%
stopwords = stopwords() | {'爱心', '流泪', '亲亲', '大笑', '首歌', '大哭', '星星'}
filtered_freq = {
    key: value for key, value in frequencies.items()
    if key not in stopwords and len(key) > 1
}

# %%
font_path = '/usr/share/fonts/noto-cjk/NotoSansCJK-Regular.ttc'
wc = wordcloud.WordCloud(
    font_path=font_path,
    background_color='white',
    # stopwords=['我', '的', '你', '是', '了', '[', ']', ',', '.', '。', ';']
    # stopwords=['爱心', '星星', '亲亲', '可爱', '色', '流泪'],
)
wc.generate_from_frequencies(filtered_freq)

wc.to_file(dest_file)

# %%
```
#### 逻辑：
先分词，根据分词结果构造词频字典  

再根据停用词，对词频字典进行过滤  

最后根据过滤后的词频字典绘制词云图  

### 3. 歌曲上传年月分布
根据结果，可见2015年歌曲上传达到极大值，2016-2020年陷入低谷，2021年以来逐步回升。  
![](./analysis/date-count.png)  
具体数据见 [data-rank.csv](./analysis/date-rank.csv)  

代码如下：
```python
# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import config
import swifter
from matplotlib import dates

# 设置全局中文字体
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = [
    'Noto Sans CJK SC',  # Google Noto 字体
    'Source Han Sans SC', # Adobe 思源黑体
    'WenQuanYi Micro Hei', # 文泉驿微米黑
    'DejaVu Sans',
    'Arial Unicode MS'
]
plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题


# %%
src_file = config.WORKSPACE + 'data/song_info.json'
df_song = pd.read_json(src_file, orient='records', lines=True)
dest_file = config.WORKSPACE + 'analysis/date-rank.csv'

# %%
df_song

# %%
def get_date(row: pd.Series) -> pd.Series:
    date = row['comments'][-1]['time'] if row['comments'] else None
    
    # exclude format such as "两天前"
    if date is not None and not date.isascii():
        date = None
    
    # 2025
    if date is not None and len(date) < 8:
        date = '2025-' + date
    
    # only reserve year and month
    # 2024-01-01 -> 2024-01
    if date is not None:
        date = date[:7]
    
    row['date'] = date
    
    return row

# %%
date_count = df_song.swifter \
                .apply(get_date, axis=1) \
                [['song_name', 'date']] \
                .dropna(axis=0) \
                .sort_values(by='date', ascending=True) \
                .groupby('date').size()
date_count.to_csv(dest_file, index=True)
# %%
date_count.index = pd.to_datetime(date_count.index)

# %%
ax = date_count.plot(figsize=(12, 6), grid=True)

# set date format 
ax.xaxis.set_major_formatter(plt.FixedFormatter(date_count.index.year.unique()))
ax.xaxis.set_major_locator(plt.MaxNLocator(len(date_count.index.year.unique())))

# plt.xticks(rotation=45)
plt.xlabel('年份')
plt.ylabel('歌曲数')
plt.title('歌曲上传年月分布')
plt.tight_layout()
plt.show()

```
#### 逻辑：
这里日期精确到月，即一月一计数。  

首先要获得歌曲上传的时间。由于爬取评论的时候获取了最早的评论，而评论带有时间信息，故这里把第一条评论的时间近似看作歌曲上传的时间。由于日期只精确到月，可以认为这里的近似是合理的。  

之后要统一时间格式。评论的时间格式依评论时间至查看时间的远近分有三种：  

第一种：一年以前的，格式为：YYYY-MM-DD  
第二种：一年之内的，略去年份，格式为：MM-DD  
第三种：一周之内，格式为：{n}天前  

第三种数量极少，这里直接忽略。第二种在前面补上"2025-"。最后，再把所有日期的"-DD"删去。这样就完成了时间格式的统一。  

之后故技重施，依照日期进行分组、计数。接下来便是绘图。  

这里为了图像的易读，把横轴设置成只显示年刻度。为此，要先把日期从字符串类型转换成 pandas 的 datetime 类型，之后再设置横轴的刻度与显示格式。  